# -*- coding: utf-8 -*-
"""HF_AutoSpeechRecgn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n2fBFVFqmdyp2b8WNQqHiUzb6RAcxO_Z
"""

!pip install transformers

!pip install librosa

import torch
import librosa as lb
import IPython.display as listen
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

# Initialize the processor
tokenizer = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')

# Initialize the model
model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')

# Listen to the clean data files from Librispeech
listen.Audio("/content/drive/MyDrive/Colab Notebooks/abc.wav", autoplay=True)

listen.Audio("/content/drive/MyDrive/Colab Notebooks/abc2.wav", autoplay=True)

# Loading the audio file
wf, sr = lb.load('/content/drive/MyDrive/Colab Notebooks/abc.wav', sr = 16000)

wf2, sr2 = lb.load('/content/drive/MyDrive/Colab Notebooks/abc2.wav', sr = 16000)

#The generated waveform is then tokenized to generate the tensors
#in the pytorch format with the argument return_tensors='pt'
input = tokenizer(wf, return_tensors='pt').input_values

input2 = tokenizer(wf2, return_tensors='pt').input_values

#The tensor is then fit in to the model to generate the non-normalized predicted values
logts = model(input).logits

logts2 = model(input2).logits

# We will take the argmax value from the logits retrieved from the model
pred = torch.argmax(logts, dim=-1)
transcription = tokenizer.batch_decode(pred)

pred2 = torch.argmax(logts2, dim=-1)
transcription2 = tokenizer.batch_decode(pred2)

# Print the transcription text for the clean audio file
print(transcription)

print(transcription2)